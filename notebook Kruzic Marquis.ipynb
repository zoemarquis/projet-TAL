{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0088cf5c-ad98-48f8-a44a-03d187b671f5",
   "metadata": {},
   "source": [
    "# Projet TAL - RI\n",
    "_Charlotte Kruzic & Zoé Marquis_\n",
    "# Étape 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f8909-77c9-4230-b554-4a24f5a1dbca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5f68b-e553-4916-a5fb-61c1c57718d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports nécessaires\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppression de l'affichage des messages d'avertissement\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import string\n",
    "import time\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn import set_config\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import seaborn as sns\n",
    "sns.set_theme(style=\"darkgrid\")\n",
    "\n",
    "set_config(display='diagram')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f59426f",
   "metadata": {},
   "source": [
    "### Charger les données & exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1788d78-79f8-4095-bfec-38ae558fff63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"train.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d74143-d69d-492d-96a0-8060a7f8f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fe3c0d-54c0-4109-bb09-fde3b6800042",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09f8c8a-73fe-4644-8466-1a37cc4ca3fc",
   "metadata": {},
   "source": [
    "### Nettoyer les données "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5145214-4410-4aa0-9b07-e06633ff0496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier les valeurs manquantes\n",
    "valeurs_manquantes = df_train.isnull().sum()\n",
    "print(valeurs_manquantes) \n",
    "# pas de valeur manquante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145957e9-5e95-4f3f-a1c0-8246285c7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifier les lignes dupliquées\n",
    "dupliquees = df_train.duplicated()\n",
    "print(dupliquees.sum())\n",
    "# 1 ligne dupliquée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9311e2-7c60-461a-a138-dcd7e43eeb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# supprimer les lignes dupliquées\n",
    "df_train = df_train.drop_duplicates()\n",
    "dupliquees = df_train.duplicated()\n",
    "print(dupliquees.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b2969-a21e-4807-a8bb-2380dfb7a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_train.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1381b66e-06c9-43da-8074-cedebe68b58c",
   "metadata": {},
   "source": [
    "Le jeu de données contient 1475 lignes.  \n",
    "Toutes les colonnes contiennent du texte (donc il n'est pas nécessaire de traiter les données numériques).   \n",
    "On s'intéresse seulement aux colonnes `headline` et `text`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90192837-a30c-4e1f-8d22-10b96d0e76d5",
   "metadata": {},
   "source": [
    "### Préparer les données pour l'apprentissage de la catégorie\n",
    "à partir du titre et du texte uniquement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b5ccb2",
   "metadata": {},
   "source": [
    "Séparation des données de la catégorie à prédire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea087ce1-7489-41a7-a0ad-66625986161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[['headline', 'text']]\n",
    "y = df_train.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c69f02-8c67-4b7c-892e-1737095e9d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d6f94-93d6-48bb-921e-38adc282bd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# afficher toutes les catégories\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b04331",
   "metadata": {},
   "source": [
    "### Prétraitements des données\n",
    "\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24225c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_into_tokens_nltk(desc) :\n",
    "#     return word_tokenize(desc, language='french')\n",
    "# \n",
    "# # Liste des mots vides de NLTK + signes de ponctuation\n",
    "# nltk_stopwords = stopwords.words('french')+list(string.punctuation)\n",
    "# # +['``'] : à essayer\n",
    "\n",
    "\n",
    "# Liste des mots vides de NLTK + signes de ponctuation\n",
    "nltk_stopwords = set(stopwords.words('french')) | set(string.punctuation) \n",
    "\n",
    "# Fonction de tokenisation personnalisée\n",
    "def split_into_tokens_nltk(desc):\n",
    "    tokens = word_tokenize(desc, language='french')\n",
    "    tokens = [token.lower() for token in tokens if token.lower() not in nltk_stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22342e9f",
   "metadata": {},
   "source": [
    "On va essayer plusieurs prétraitements différents supplémentaires dont : \n",
    "- Lemmatisation\n",
    "- Désuffixation\n",
    "- N-grammes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='french')\n",
    "\n",
    "def stemming(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "\n",
    "def stemming_transformer(X):\n",
    "    return X.apply(stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed2f0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatize_text(text):\n",
    "#     # modèle français de Spacy\n",
    "#     nlp = spacy.load(\"fr_core_news_sm\")\n",
    "#     lemmatized_text = ' '.join([token.lemma_ for token in nlp(text)])\n",
    "#     return lemmatized_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text, language='french')\n",
    "    lemmatized_text = ' '.join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "    return lemmatized_text\n",
    "\n",
    "def lemmatization_transformer(X):\n",
    "    return X.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660fcfa7",
   "metadata": {},
   "source": [
    "On va essayer 3 types de traits : \n",
    "- TF-IDF (avec différentes fréquences)\n",
    "- Sac de mots (avec différentes fréquences)\n",
    "- Informations statistiques : longueur en nombre de caractères et nombre approximatif de phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56fdabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stats(descriptions):\n",
    "    return [{\"length\": len(text), \"num_sentences\": text.count(\".\")}\n",
    "            for text in descriptions]\n",
    "    \n",
    "text_stats_transformer = FunctionTransformer(text_stats)\n",
    "text_stats_vectorizer = DictVectorizer(sparse=False)\n",
    "min_max_scaler = MinMaxScaler()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_001 = TfidfVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                            lowercase=True, \n",
    "                            # stop_words=nltk_stopwords,\n",
    "                            min_df = 0.01)\n",
    "bow_001 = CountVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                          lowercase=True,\n",
    "                          # stop_words=nltk_stopwords,\n",
    "                          min_df=0.01)\n",
    "\n",
    "tfidf_001_ngram2 = TfidfVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                            lowercase=True, \n",
    "                            # stop_words=nltk_stopwords,\n",
    "                            min_df = 0.01,\n",
    "                            ngram_range=(1, 2)\n",
    "                            )\n",
    "bow_001_ngram2 = CountVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                          lowercase=True,\n",
    "                          # stop_words=nltk_stopwords,\n",
    "                          ngram_range=(1, 2),\n",
    "                          min_df=0.01)\n",
    "\n",
    "tfidf_001_ngram3 = TfidfVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                            lowercase=True, \n",
    "                            # stop_words=nltk_stopwords,\n",
    "                            min_df = 0.01,\n",
    "                            ngram_range=(1, 3)\n",
    "                            )\n",
    "bow_001_ngram3 = CountVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                          lowercase=True,\n",
    "                          # stop_words=nltk_stopwords,\n",
    "                          ngram_range=(1, 3),\n",
    "                          min_df=0.01)\n",
    "\n",
    "tfidf_001_ngram4 = TfidfVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                            lowercase=True, \n",
    "                            # stop_words=nltk_stopwords,\n",
    "                            min_df = 0.01,\n",
    "                            ngram_range=(1, 4)\n",
    "                            )\n",
    "bow_001_ngram4 = CountVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                          lowercase=True,\n",
    "                          # stop_words=nltk_stopwords,\n",
    "                          ngram_range=(1, 4),\n",
    "                          min_df=0.01)\n",
    "\n",
    "tfidf_010 = TfidfVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                            lowercase=True, \n",
    "                            # stop_words=nltk_stopwords,\n",
    "                            min_df = 0.1)\n",
    "bow_010 = CountVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                          lowercase=True,\n",
    "                          # stop_words=nltk_stopwords,\n",
    "                          min_df=0.1)\n",
    "\n",
    "tfidf_005 = TfidfVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                            lowercase=True, \n",
    "                            # stop_words=nltk_stopwords,\n",
    "                            min_df = 0.05)\n",
    "bow_005 = CountVectorizer(tokenizer=split_into_tokens_nltk,\n",
    "                            token_pattern=None,\n",
    "                          lowercase=True,\n",
    "                         #  stop_words=nltk_stopwords,\n",
    "                          min_df=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf5a446",
   "metadata": {},
   "source": [
    "### Modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca62beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Baseline', DummyClassifier(strategy='most_frequent')),\n",
    "    ('Mutinomial NB', MultinomialNB()),\n",
    "    ('CART', DecisionTreeClassifier()),\n",
    "    ('LR', LogisticRegression()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('Random forest', RandomForestClassifier()),\n",
    "    ('SVM', SVC(kernel='rbf')),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c18a8",
   "metadata": {},
   "source": [
    "### Évaluation\n",
    "\n",
    "Validation croisée à 5 plis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a725fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,\n",
    "                                                    random_state=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08862b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "stratkfold = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e2e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(modele, trait):\n",
    "    model_pipeline = make_pipeline(trait, modele)\n",
    "    y_pred = model_selection.cross_val_predict(model_pipeline, X_train, y_train,\n",
    "                                               cv=stratkfold, n_jobs=-1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a186dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(chaines, modeles):\n",
    "    classification_reports = {}\n",
    "\n",
    "    for identifiant, colonne_transformateur in chaines:\n",
    "        classification_reports[identifiant] = {}\n",
    "        \n",
    "        for name, model in models :\n",
    "            y_pred = evaluate(model, trait=colonne_transformateur)\n",
    "            \n",
    "            report = classification_report(y_train, y_pred, output_dict=True)\n",
    "            \n",
    "            classification_reports[identifiant][name] = {\n",
    "                \"accuracy\" : report[\"accuracy\"],\n",
    "                \"f1-score\" : report[\"macro avg\"][\"f1-score\"],\n",
    "                \"precision\" : report[\"macro avg\"][\"precision\"],\n",
    "                \"recall\" : report[\"macro avg\"][\"recall\"],\n",
    "            }\n",
    "        \n",
    "    return classification_reports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44656440",
   "metadata": {},
   "source": [
    "### Visualiser pour comparer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9678be7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def afficher_chaines(reports):\n",
    "    for chaine_num, chaine_data in reports.items():\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "        fig.suptitle(chaine_num) \n",
    "\n",
    "        metrics = ['accuracy', 'f1-score', 'precision', 'recall']\n",
    "\n",
    "        for ax, metric in zip(axs.flatten(), metrics):\n",
    "            model_names = list(chaine_data.keys())\n",
    "            metric_values = [model_data[metric] for model_data in chaine_data.values()]\n",
    "            \n",
    "            \n",
    "            mean_metric = np.mean(metric_values)\n",
    "            max_metric = np.max(metric_values)\n",
    "            \n",
    "            ax.axvline(mean_metric, color='r', linestyle='--', label=f'Mean ({mean_metric:.2f})')\n",
    "            ax.axvline(max_metric, color='g', linestyle='--', label=f'Max ({max_metric:.2f})')\n",
    "            \n",
    "\n",
    "            ax.barh(model_names, metric_values, color='b')\n",
    "            ax.set_title(metric.upper()) \n",
    "            ax.set_xlabel(metric.capitalize())  \n",
    "            ax.set_ylabel('Modèles')  \n",
    "            ax.invert_yaxis()  \n",
    "            ax.set_xlim(0, 1)\n",
    "            \n",
    "            ax.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a4715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparer_chaines(reports):\n",
    "    metrics = ['accuracy', 'f1-score', 'precision', 'recall']\n",
    "    metric_values = {metric: [] for metric in metrics}\n",
    "\n",
    "    for chaine_num, chaine_data in reports.items():\n",
    "        longueur = len(chaine_data.keys())\n",
    "        for model, values in chaine_data.items():\n",
    "            for metric in metrics:\n",
    "                metric_values[metric].append((chaine_num, model, values[metric]))\n",
    "                \n",
    "\n",
    "    for metric in metrics:\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(8, 6))\n",
    "        fig.suptitle(f'Comparaison de la métrique {metric.capitalize()} par modèle')  # Titre de la figure\n",
    "\n",
    "        for i, ax in enumerate(axs.flat):\n",
    "            values = metric_values[metric][i*longueur:(i+1)*longueur]\n",
    "\n",
    "            chaine_names = [value[0] for value in values]\n",
    "            model_names = [value[1] for value in values]\n",
    "            metric_scores = [value[2] for value in values]\n",
    "            \n",
    "            mean_metric_score = np.mean(metric_scores)\n",
    "            max_metric_score = np.max(metric_scores)\n",
    "            \n",
    "            ax.axvline(mean_metric_score, color='r', linestyle='--', label=f'Moyenne ({mean_metric_score:.2f})')\n",
    "            ax.axvline(max_metric_score, color='g', linestyle='--', label=f'Maximum ({max_metric_score:.2f})')\n",
    "\n",
    "            ax.barh(model_names, metric_scores, color='b')\n",
    "            ax.set_title(chaine_names[0])  \n",
    "            ax.set_xlabel(chaine_names[0]) \n",
    "            ax.set_ylabel('Chaines')  \n",
    "            ax.invert_yaxis() \n",
    "            ax.set_xlim(0, 1)\n",
    "            ax.legend()\n",
    "            \n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672cd010",
   "metadata": {},
   "source": [
    "## Premières chaines\n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text |\n",
    "| --- | --- | --- |\n",
    "| 1 | tf-idf (fréquence = 0.01) | tf-idf (fréquence = 0.01) | \n",
    "| 2 | sac de mots (fréquence = 0.01) | sac de mots (fréquence = 0.01) | \n",
    "| 3 | tf-idf (fréquence = 0.01) |  sac de mots (fréquence = 0.01) | \n",
    "| 4 | sac de mots (fréquence = 0.01) |  tf-idf (fréquence = 0.01) | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06bf3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain1', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_tfidf_001', tfidf_001, 'text'),\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain2', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_bow_001', bow_001, 'headline'),\n",
    "            ('text_bow_001', bow_001, 'text'),\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain3', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain4', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_bow_001', bow_001, 'headline'),\n",
    "            ('text_tfidf_001', tfidf_001, 'text'),\n",
    "        ]\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37948d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae64f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "afficher_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e629b",
   "metadata": {},
   "source": [
    "On s'aperçoit tout de suite que les 4 meilleurs modèles sont (presque) à chaque fois \n",
    "- Multinomial NB\n",
    "- LR (Régression Logistique)\n",
    "- Random forest\n",
    "- SVM\n",
    "\n",
    "Nous redéfinissons donc la liste des modèles que nous utiliserons : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Mutinomial NB', MultinomialNB()),\n",
    "    ('LR', LogisticRegression()),\n",
    "    ('Random forest', RandomForestClassifier()),\n",
    "    ('SVM', SVC(kernel='rbf')),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a63c4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38ef2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc766dec",
   "metadata": {},
   "source": [
    "On élimine les chaines 1 et 4 car elles ont un F1-score moins bons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4d8cfe",
   "metadata": {},
   "source": [
    "Essayons maintenant d'ajouter les traits d'informations statistiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a1510",
   "metadata": {},
   "source": [
    "## Deuxièmes chaines\n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text |\n",
    "| --- | --- | --- |\n",
    "| 2 | sac de mots (fréquence = 0.01) | sac de mots (fréquence = 0.01) | \n",
    "| 3 | tf-idf (fréquence = 0.01) |  sac de mots (fréquence = 0.01) | \n",
    "| 5 | sac de mots (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | \n",
    "| 6 | tf-idf (fréquence = 0.01) |  sac de mots (fréquence = 0.01) + informations statistiques | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bb6728",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain2', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_bow_001', bow_001, 'headline'),\n",
    "            ('text_bow_001', bow_001, 'text'),\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain3', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain5', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_bow_001', bow_001, 'headline'),\n",
    "            ('text_bow_001', bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain6', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172cf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71017932",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950293bc",
   "metadata": {},
   "source": [
    "Les mesures sont peu impactées par ce changement de traits.  \n",
    "On garde la chaine 6 pour la suite car elle est légèrement meilleure lorsqu'il s'agit de Precision Rappel (pour la valeur maximale).   \n",
    "On supprime aussi le modèle SVM qui n'est pas le plus performant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8712d713",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('Mutinomial NB', MultinomialNB()),\n",
    "    ('LR', LogisticRegression()),\n",
    "    ('Random forest', RandomForestClassifier()),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6150a903",
   "metadata": {},
   "source": [
    "## Troisièmes chaines\n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "- **N-grammes**\n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text | n-gramme |\n",
    "| --- | --- | --- | --- |\n",
    "| 6 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | \n",
    "| 7 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | 2 |\n",
    "| 8 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | 3 |\n",
    "| 9 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | 4 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cc218",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain6', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain7', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001_ngram2, 'headline'),\n",
    "            ('text_bow_001',  bow_001_ngram2, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain8', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001_ngram3, 'headline'),\n",
    "            ('text_bow_001',  bow_001_ngram3, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain9', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001_ngram4, 'headline'),\n",
    "            ('text_bow_001',  bow_001_ngram4, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb06515",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bb88af",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635a3862",
   "metadata": {},
   "source": [
    "On garde la chaine 6 qui a les mêmes mesures que la chaine 9, supérieures aux autres chaines en terme de Précision, identiques pour les autres mesures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f13f76b",
   "metadata": {},
   "source": [
    "On essaye les n-grammes lorsq'uon utilise TF-IDF pour le texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa834c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain6', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain7tfidf', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001_ngram2, 'headline'),\n",
    "            ('text_bow_001',  tfidf_001_ngram2, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain8tfidf', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001_ngram3, 'headline'),\n",
    "            ('text_bow_001',  tfidf_001_ngram3, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain9tfidf', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001_ngram4, 'headline'),\n",
    "            ('text_bow_001',  tfidf_001_ngram4, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62c75e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ec05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ec74dd",
   "metadata": {},
   "source": [
    "On garde la chaine 6 : l'*accuracy* est identique mais le F1-score est moins bon."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ca4b0",
   "metadata": {},
   "source": [
    "## Quatrièmes chaines \n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "- **Fréquences** \n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text | \n",
    "| --- | --- | --- | \n",
    "| 6 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | \n",
    "| 10 | tf-idf (fréquence = 0.1) | sac de mots (fréquence = 0.1) + informations statistiques | \n",
    "| 11 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.1) + informations statistiques | \n",
    "| 12 | tf-idf (fréquence = 0.1) | sac de mots (fréquence = 0.01) + informations statistiques | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e9b96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain6', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain10', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  bow_010, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain11', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_010, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain12', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1582c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab9ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a438dd6",
   "metadata": {},
   "source": [
    "On élimine les chaines 10 et 11 et on essaye une autre valeur pour la fréquence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959c77c",
   "metadata": {},
   "source": [
    "## Cinquièmes chaines \n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "- **Fréquences** \n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text | \n",
    "| --- | --- | --- | \n",
    "| 6 | tf-idf (fréquence = 0.01) | sac de mots (fréquence = 0.01) + informations statistiques | \n",
    "| 12 | tf-idf (fréquence = 0.1) | sac de mots (fréquence = 0.01) + informations statistiques | \n",
    "| 13 | tf-idf (fréquence = 0.05) | sac de mots (fréquence = 0.05) + informations statistiques | \n",
    "| 14 | tf-idf (fréquence = 0.1) | sac de mots (fréquence = 0.05) + informations statistiques | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76399db",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain6', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_001, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain12', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    \n",
    "    ('chain13', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_005, 'headline'),\n",
    "            ('text_bow_001',  bow_005, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain14', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  bow_005, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c318fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e4dd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd23275f",
   "metadata": {},
   "source": [
    "On conserve la chaine 12 qui obtient les mêmes scores pour toutes les mesures, et est légèrement supérieure en terme de Rappel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2508d264",
   "metadata": {},
   "source": [
    "## Sixièmes chaines \n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "- **Désuffixation**\n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text | \n",
    "| --- | --- | --- | \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e447fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain12', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    ('chain15', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', \n",
    "                Pipeline(\n",
    "                 [\n",
    "                     ('stemming', FunctionTransformer(stemming_transformer)),\n",
    "                     ('text_stats', tfidf_010),\n",
    "                 ]\n",
    "                ),\n",
    "                 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    ('chain16', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  \n",
    "                Pipeline(\n",
    "                 [\n",
    "                     ('stemming', FunctionTransformer(stemming_transformer)),\n",
    "                     ('text_stats', bow_001),\n",
    "                 ]\n",
    "                ),\n",
    "                 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    ('chain17', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_001', \n",
    "                Pipeline(\n",
    "                 [\n",
    "                     ('stemming', FunctionTransformer(stemming_transformer)),\n",
    "                     ('text_stats', tfidf_010),\n",
    "                 ]\n",
    "                ),\n",
    "                 'headline'),\n",
    "            ('text_bow_001',  \n",
    "                Pipeline(\n",
    "                 [\n",
    "                     ('stemming', FunctionTransformer(stemming_transformer)),\n",
    "                     ('text_stats', bow_001),\n",
    "                 ]\n",
    "                ),\n",
    "                 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d548aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd64f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e219f9b",
   "metadata": {},
   "source": [
    "Les chaines 12 et 15 sont légèrement meilleures en terme de précision, on conserve la 12."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165b19a6",
   "metadata": {},
   "source": [
    "## Septièmes chaines \n",
    "\n",
    "Le pré-traitement appliqué est :\n",
    "- Nettoyage du texte : supprimer les caractères spéciaux, les ponctuations, les accents convertir en minuscules\n",
    "- Tokenisation\n",
    "- Suppression des mots vides\n",
    "- Désuffixation\n",
    "- Lemmatisation\n",
    "\n",
    "Et on utilise les traits suivants :\n",
    "\n",
    "| n° | headline | text | \n",
    "| --- | --- | --- | \n",
    "| 12 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1162d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chaines = [\n",
    "    ('chain12', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_010', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain18', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_010', \n",
    "                Pipeline(\n",
    "                 [\n",
    "                     ('lemmatization',  FunctionTransformer(lemmatization_transformer)),\n",
    "                     ('text_stats', tfidf_010),\n",
    "                 ]\n",
    "                ),\n",
    "                 'headline'),\n",
    "            ('text_bow_001',  bow_001, 'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain19', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_010', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  \n",
    "              Pipeline(\n",
    "                 [\n",
    "                     ('lemmatization',  FunctionTransformer(lemmatization_transformer)),\n",
    "                     ('text_stats', bow_001),\n",
    "                 ]\n",
    "                ),\n",
    "              'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "    \n",
    "    ('chain20', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_010', \n",
    "                Pipeline(\n",
    "                 [\n",
    "                     ('lemmatization',  FunctionTransformer(lemmatization_transformer)),\n",
    "                     ('text_stats', tfidf_010),\n",
    "                 ]\n",
    "                ),\n",
    "                 'headline'),\n",
    "            ('text_bow_001',  \n",
    "              Pipeline(\n",
    "                 [\n",
    "                     ('lemmatization',  FunctionTransformer(lemmatization_transformer)),\n",
    "                     ('text_stats', bow_001),\n",
    "                 ]\n",
    "                ),\n",
    "              'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    )),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afb6959",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_reports = run(chaines, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac2dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparer_chaines(classification_reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f622375e",
   "metadata": {},
   "source": [
    "Finalement, on conserve le modèle Multinomial NB car il nous permet de sélectionner l'accuracy et le F1-score maximaux.\n",
    "\n",
    "Inspection des valeurs pour les 4 chaines :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a991c4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_dict_values(d):\n",
    "    return {k: round(v, 3) for k, v in d.items()}\n",
    "\n",
    "print(\"12\",classification_reports[\"chain12\"][\"Mutinomial NB\"])\n",
    "print(\"18\",classification_reports[\"chain18\"][\"Mutinomial NB\"])\n",
    "print(\"19\",classification_reports[\"chain19\"][\"Mutinomial NB\"])\n",
    "print(\"20\",classification_reports[\"chain20\"][\"Mutinomial NB\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5efcc",
   "metadata": {},
   "source": [
    "On sélectionne donc finalement la chaine 19 (scores égaux à 20 mais 20 lemmatise en plus le titre, ce que ne fait pas 19).\n",
    "\n",
    "La combinaison sélectionnée est donc \n",
    "- modèle = Multinomial NB\n",
    "- traits = \n",
    "  - TF-IDF, fréquence 0.1 pour le titre\n",
    "    - Prétraitement :\n",
    "      - Nettoyage du texte\n",
    "      - Tokenisation\n",
    "      - Suppression des mots vides\n",
    "  - Sac de mots, fréquence 0.01 pour le texte\n",
    "    - Prétraitement : \n",
    "      - Nettoyage du texte\n",
    "      - Tokenisation\n",
    "      - Suppression des mots vides\n",
    "      - Lemmatisation\n",
    "  - Informations statistiques sur le texte\n",
    "    - Longueur du texte en nombre de caractères, nombre de phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cb0ff",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016ad8f4",
   "metadata": {},
   "source": [
    "## BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3273b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a711f",
   "metadata": {},
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79efc346",
   "metadata": {},
   "source": [
    "Les dossiers suivant contiennent les vecteurs de mots pré-entrainés utilisés, ils doivent être téléchargés et dézippés :\n",
    "- https://nlp.stanford.edu/data/glove.6B.zip\n",
    "- https://nlp.stanford.edu/data/glove.42B.300d.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24c2be1",
   "metadata": {},
   "source": [
    "#### Corpus annoté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d775a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture du fichier\n",
    "df = pd.read_csv(\"train.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661f86ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des classes\n",
    "class_names = sorted(df['category'].unique())\n",
    "print(\"Classes :\", class_names)\n",
    "print(\"Nombre d'exemplaires :\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dc8f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On associe à chaque classe un identifiant unique\n",
    "class_index = {class_names[i]:i for i in range(len(class_names))}\n",
    "class_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c7f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[\"headline\"] + ' ' + df[\"text\"]\n",
    "#Remplacement des noms des classes par un entier\n",
    "y_train = df[\"category\"].map(class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d943008",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectorizer(documents, max_voc_size=8000, max_seq_length=50, batch_size=128):\n",
    "  vectorizer = TextVectorization(max_tokens=max_voc_size,\n",
    "                                 output_sequence_length=max_seq_length)\n",
    "  text_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n",
    "  vectorizer.adapt(text_ds)\n",
    "  return vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82388e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_vectorizer = get_vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1c783c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = keras_vectorizer.get_vocabulary()\n",
    "word_index = dict(zip(voc, range(len(voc))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f292a8",
   "metadata": {},
   "source": [
    "#### Plongements de mots pré-entraînés\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d61671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(embeddings_file):\n",
    "  embeddings_index = {}\n",
    "  with open(embeddings_file, 'r', encoding='utf8') as f:\n",
    "      for line in f:\n",
    "          word, coefs = line.split(maxsplit=1)\n",
    "          coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "          embeddings_index[word] = coefs\n",
    "  print(f'{len(embeddings_index)} vecteurs de mots ont été lus')\n",
    "  return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48171cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des plongements du fichier Glove\n",
    "glove_embeddings = load_embeddings('glove.42B.300d/glove.42B.300d.txt')\n",
    "#glove_embeddings = load_embeddings('glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1345b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix(vocabulary, embeddings_index, embedding_dim = 300):\n",
    "  num_tokens = len(vocabulary)\n",
    "  hits = 0\n",
    "  misses = 0\n",
    "\n",
    "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "  for word, i in word_index.items():\n",
    "      embedding_vector = embeddings_index.get(word)\n",
    "      if embedding_vector is not None:\n",
    "          embedding_matrix[i] = embedding_vector\n",
    "          hits += 1\n",
    "      else:\n",
    "          misses += 1\n",
    "  print(f'{hits} mots ont été trouvés dans les plongements pré-entraînés')\n",
    "  print(f'{misses} sont absents')\n",
    "  return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812970bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de la matrice de plongements à partir du vocabulaire\n",
    "glove_embedding_matrix = get_embedding_matrix(voc, glove_embeddings)\n",
    "\n",
    "#glove.42B.300d/glove.42B.300d.txt\n",
    "#6147 mots ont été trouvés dans les plongements pré-entraînés\n",
    "#1853 sont absents\n",
    "\n",
    "#glove.6B/glove.6B.300d.txt\n",
    "#3619 mots ont été trouvés dans les plongements pré-entraînés\n",
    "#4381 sont absents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5feedc1",
   "metadata": {},
   "source": [
    "### Construction et entraînement du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_biLSTM_model(voc_size, embedding_matrix, embedding_dim=300):\n",
    "  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "  embedding_layer = Embedding(voc_size, embedding_dim, trainable=True,\n",
    "      embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "  )\n",
    "\n",
    "  embedded_sequences = embedding_layer(int_sequences_input)\n",
    "  x = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(embedded_sequences)\n",
    "  preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "  model = keras.Model(int_sequences_input, preds)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c18768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage de l'architecture du modèle\n",
    "biLSTM_model = get_biLSTM_model(len(voc), glove_embedding_matrix)\n",
    "biLSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625030a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour l'entraînement d'un modèle\n",
    "def train_model(X, y, model_function, vectorizer,\n",
    "                voc_size, embedding_matrix, embedding_dim=300, batch_size=128):\n",
    "  acc_per_fold = []\n",
    "  loss_per_fold = []\n",
    "  histories = []\n",
    "  folds = 5\n",
    "  stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True,\n",
    "                                              random_state=12)\n",
    "  fold_no = 1\n",
    "  for train, test in stratkfold.split(X, y):\n",
    "    m_function = globals()[model_function]\n",
    "    model = m_function(voc_size, embedding_matrix, embedding_dim)\n",
    "\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Entraînement pour le pli {fold_no} ...')\n",
    "    fold_x_train = vectorizer(X.iloc[train].to_numpy()).numpy()\n",
    "    fold_x_val = vectorizer(X.iloc[test].to_numpy()).numpy()\n",
    "    fold_y_train = y.iloc[train].to_numpy()\n",
    "    fold_y_val = y.iloc[test].to_numpy()\n",
    "    \n",
    "    model.compile(\n",
    "      loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n",
    "    )\n",
    "    # Entraînement\n",
    "    history = model.fit(fold_x_train, fold_y_train, batch_size=batch_size,\n",
    "                        epochs=10, validation_data=(fold_x_val, fold_y_val))\n",
    "    histories.append(history)\n",
    "    # Evaluation sur les données de validation\n",
    "    scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n",
    "    print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n",
    "          f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n",
    "    acc_per_fold.append(scores[1] * 100)\n",
    "    loss_per_fold.append(scores[0])\n",
    "    fold_no = fold_no + 1\n",
    "\n",
    "  # Affichage des scores moyens par pli\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print('Scores par pli')\n",
    "  for i in range(0, len(acc_per_fold)):\n",
    "    print('---------------------------------------------------------------------')\n",
    "    print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n",
    "          f'- Accuracy: {acc_per_fold[i]:.2f}%')\n",
    "  print('---------------------------------------------------------------------')\n",
    "  print('Scores moyens pour tous les plis :')\n",
    "  print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n",
    "        f'(+- {np.std(acc_per_fold):.2f})')\n",
    "  print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n",
    "  print('---------------------------------------------------------------------')\n",
    "  return histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d55524",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Séparation données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement du modèle\n",
    "biLSTM_histories = train_model(X_train, y_train, 'get_biLSTM_model',\n",
    "                            keras_vectorizer, len(voc), glove_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e2b0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc2a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(histories):\n",
    "  accuracy_data = []\n",
    "  loss_data = []\n",
    "  for i, h in enumerate(histories):\n",
    "    acc = h.history['acc']\n",
    "    val_acc = h.history['val_acc']\n",
    "    loss = h.history['loss']\n",
    "    val_loss = h.history['val_loss']\n",
    "    for j in range(len(acc)):\n",
    "      accuracy_data.append([i+1, j+1, acc[j], 'Entraînement'])\n",
    "      accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n",
    "      loss_data.append([i+1, j+1, loss[j], 'Entraînement'])\n",
    "      loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n",
    "\n",
    "  acc_df = pd.DataFrame(accuracy_data,\n",
    "                        columns=['Pli', 'Epoch', 'Accuracy', 'Données'])\n",
    "  sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Données',\n",
    "              kind='line')\n",
    "\n",
    "  loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Données'])\n",
    "  sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Données',\n",
    "              kind='line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8703e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(biLSTM_histories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bff339",
   "metadata": {},
   "source": [
    "### Prédictions avec le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ddd294",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectorized = keras_vectorizer(X_test.to_numpy()).numpy()\n",
    "y_pred_prob = biLSTM_model.predict(X_test_vectorized)\n",
    "y_pred_indices = np.argmax(y_pred_prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rapport de classification\n",
    "report = classification_report(y_test, y_pred_indices, target_names=class_names)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57818c22",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7039572f",
   "metadata": {},
   "source": [
    "## Application du meilleur modèle aux données de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5df21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chargement du jeu de données de test\n",
    "df_test = pd.read_csv(\"test.tsv\", sep='\\t')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575002c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "('chain19', ColumnTransformer(\n",
    "        [\n",
    "            ('headline_tfidf_010', tfidf_010, 'headline'),\n",
    "            ('text_bow_001',  \n",
    "              Pipeline(\n",
    "                 [\n",
    "                     ('lemmatization',  FunctionTransformer(lemmatization_transformer)),\n",
    "                     ('text_stats', bow_001),\n",
    "                 ]\n",
    "                ),\n",
    "              'text'),\n",
    "            (\n",
    "             'text_stats',\n",
    "             Pipeline(\n",
    "                 [\n",
    "                     ('text_stats', text_stats_transformer),\n",
    "                     ('vect', text_stats_vectorizer),\n",
    "                     ('scaling', min_max_scaler)\n",
    "                 ]\n",
    "             ),\n",
    "             'text'\n",
    "         )\n",
    "        ]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0eafa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
