{"cells":[{"cell_type":"markdown","source":["# Utilisation de transformers pour la classification de textes\n","\n"," > ‚ÑπÔ∏è Inspir√© de :\n"," > - https://github.com/nlp-with-transformers/notebooks/blob/main/02_classification.ipynb\n"," > - https://huggingface.co/docs/transformers/tasks/sequence_classification\n","\n","ü•Ö **Objectifs**\n","\n","- Savoir utiliser l'√©cosyst√®me HuggingFace pour r√©utiliser des mod√®les pr√©-entra√Æn√©s et les affiner sur de nouvelles donn√©es"],"metadata":{"id":"gi9e0LXqqQSQ"}},{"cell_type":"markdown","source":["## 1. Installation des librairies n√©cessaires"],"metadata":{"id":"CcRN0X6ErIdd"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nOiT_b0Kie1S"},"outputs":[],"source":["! pip install transformers[torch]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DxMx3WDolLON"},"outputs":[],"source":["! pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDDnIa9TTz6F"},"outputs":[],"source":["! pip install evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZpuVC6BkiaLz"},"outputs":[],"source":["from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification, TrainingArguments, Trainer\n","from datasets import Features, Value, ClassLabel, Dataset, DatasetDict\n","import evaluate\n","import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score, f1_score\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"JGZew8OkiI-b"},"source":["## 2. R√©cup√©ration et pr√©paration des donn√©es"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VguESMbzgaD1"},"outputs":[],"source":["!mkdir data\n","!wget -P data https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/winemag-fr_train.csv"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGfyhZ1_h6qE"},"outputs":[],"source":["wine_df = pd.read_csv(\"data/winemag-fr_train.csv\", sep=\",\", dtype={'description': 'object',\n","                                           'price': 'float64',\n","                                           'province': 'category',\n","                                           'variety': 'object'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XbevtRBGh9cO"},"outputs":[],"source":["# Liste de classes et ajout d'un identifiant num√©rique pour chaque classe\n","class_names = sorted(wine_df.province.unique().categories.to_list())\n","label2id = {class_names[i]:i for i in range(len(class_names))}\n","id2label = {i:class_names[i] for i in range(len(class_names))}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F3rK4POliEL5"},"outputs":[],"source":["data_df = pd.DataFrame()\n","# Le texte d√©crivant chaque vin est compos√© des colonnes variety et description\n","split_variety = wine_df.variety.str.split('_')\n","data_df['text'] = split_variety.str.join(' ') + ' ' + wine_df.description\n","# La classe cible est la r√©gion (province) sous forme d'identifiant num√©rique\n","data_df['label'] = wine_df.province.map(label2id)\n","\n","# Transformation du DataFrame en objet de type Dataset utilis√© par HuggingFace\n","province_features = Features({'text': Value('string'),\n","                              'label': ClassLabel(names=class_names)})\n","data = Dataset.from_pandas(data_df, features=province_features)\n","# D√©coupage en train et test\n","data = data.train_test_split(test_size=0.2, shuffle=True, seed=12)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LG5q5Iknl_G_"},"outputs":[],"source":["data['train'].features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xg9o8kTmVdG"},"outputs":[],"source":["data['train'][0]"]},{"cell_type":"markdown","source":["## 3. Tok√©nisation des donn√©es\n","\n","Nous allons utiliser une variante de BERT (pour l'anglais) appel√©e DistilBERT. Ce mod√®le obtient des performances comparables √† BERT, mais est de plus petite taille et plus rapide.\n","\n","üö® **DistilBERT est un mod√®le pour l'anglais et ne doit donc pas √™tre utilis√© pour des textes dans une autre langue. Pour rechercher des mod√®les adapt√©s √† une autre langue, utiliser le filtre \"Languages\" sur https://huggingface.co/models** üö®"],"metadata":{"id":"szqvBNSfsDSk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FJciqCVmohf"},"outputs":[],"source":["model_ckpt = \"distilbert-base-uncased\"\n","# Chargement du tok√©niseur pr√©-entra√Æn√© correspondant au mod√®le utilis√©\n","tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SOFUysLJm2T1"},"outputs":[],"source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], padding=True, truncation=True)"]},{"cell_type":"code","source":["# Tok√©nisation des 2 premi√®res instances\n","preprocess_function(data['train'][:2])"],"metadata":{"id":"TvzPMhsmxLlW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- Les 0 √† la fin sont le r√©sultat du padding (toutes les s√©quences du lot ont la m√™me longueur)\n","- Les 0 dans le masque d'attention indiquent les tokens √† ignorer dans le m√©canisme d'attention (tokens ajout√©s par le padding)"],"metadata":{"id":"SjtNIm1UyCaY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"itrjXWHxUQTd"},"outputs":[],"source":["# Tokenisation de la totalit√© des donn√©es : chaque unit√© est remplac√©e par un identifiant num√©rique\n","tokenized_data = data.map(preprocess_function, batched=True, batch_size=None)"]},{"cell_type":"code","source":["tokenized_data['train'][0]"],"metadata":{"id":"fhwa8p1lXeRd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Affichage des tokens. DistilBERT utilise l'algorithme WordPiece\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_data['train'][0]['input_ids'])\n","print(tokenized_data['train'][0]['text'])\n","print(tokens)"],"metadata":{"id":"_4hFw4kAhUyd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- [CLS] et [SEP] indiquent le d√©but et la fin de la s√©quence.\n","- Les tokens sont en minuscules.\n","- Le pr√©fixe ## indique que le sous-mot n'est pas pr√©c√©d√© par une espace"],"metadata":{"id":"Eshh-bpKvSLY"}},{"cell_type":"code","source":["# Taille du vocabulaire\n","tokenizer.vocab_size"],"metadata":{"id":"aS0c4D5DiC0_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Taille de contexte maximum\n","tokenizer.model_max_length"],"metadata":{"id":"guuWBiM9iGx3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4. Pr√©paration de l'√©valuation"],"metadata":{"id":"cq1sKH480AJH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aw6bbkwVnX9t"},"outputs":[],"source":["accuracy = evaluate.load(\"accuracy\")"]},{"cell_type":"code","source":["f1_metric = evaluate.load(\"f1\")"],"metadata":{"id":"0r8ydHKa3mgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9KHsFWIneX1"},"outputs":[],"source":["def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    acc = accuracy.compute(predictions=predictions, references=labels)\n","    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n","    return {\"accuracy\": acc['accuracy'], \"f1-macro\": f1[\"f1\"]}"]},{"cell_type":"markdown","source":["## 5. Entra√Ænement par affinage\n","\n","On commence par charger le mod√®le pr√©-entra√Æn√©"],"metadata":{"id":"LbGHVSMK0HEG"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-vjhGFp5PKl1"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yvcqj_DxPV23"},"outputs":[],"source":["batch_size = 64\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_ckpt}-finetuned-wine\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    num_train_epochs=5,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJ5aXto6PeLN"},"outputs":[],"source":["data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","def init_trainer():\n","  model = AutoModelForSequenceClassification.from_pretrained(\n","    model_ckpt, num_labels=len(class_names), id2label=id2label, label2id=label2id\n","    ).to(device)\n","  return Trainer(\n","      model=model,\n","      args=training_args,\n","      train_dataset=tokenized_data[\"train\"],\n","      eval_dataset=tokenized_data[\"test\"],\n","      tokenizer=tokenizer,\n","      data_collator=data_collator,\n","      compute_metrics=compute_metrics,\n","  ), model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SzLipK_TQgW"},"outputs":[],"source":["trainer, model = init_trainer()\n","trainer.train()"]},{"cell_type":"markdown","source":["‚ùì [1] Que constatez-vous par rapport aux r√©sultats obtenus pr√©c√©demment pour ce jeu de donn√©es ? Attention, ici nous ne faisons pas de validation crois√©e √† 5 plis, les r√©sultats sont √©valu√©s uniquement sur 20% des donn√©es (un seul pli)."],"metadata":{"id":"7C6O3JW61RW2"}},{"cell_type":"code","source":["trainer2, model2 = init_trainer()\n","trainer2.train()"],"metadata":{"id":"_uR11EfCSGt_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚ùì [2] Que constatez-vous par rapport aux r√©sultats d'entra√Ænement obtenus pour la cellule pr√©c√©dente ? Est-ce que les r√©sultats sont les m√™mes ?"],"metadata":{"id":"ot01spIwg8Gv"}},{"cell_type":"markdown","source":["## 6. Analyse des r√©sultats"],"metadata":{"id":"nzOxIvQs2FtX"}},{"cell_type":"code","source":["# Pr√©dictions pour les donn√©es de test\n","preds_output = trainer.predict(tokenized_data['test'])"],"metadata":{"id":"x8QTyropfF3t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_output"],"metadata":{"id":"PYpII9EOj7Mh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["preds_output.metrics"],"metadata":{"id":"qB8Yud4YfOMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","Nous allons √©galement afficher la matrice de confusion.\n","\n","\n","\n"],"metadata":{"id":"V9VGKl6U2cL1"}},{"cell_type":"code","source":["y_preds = np.argmax(preds_output.predictions, axis=1)"],"metadata":{"id":"SK7EkwKZfS7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_valid = tokenized_data['test']['label']"],"metadata":{"id":"kW8oaIAhf-Cg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = tokenized_data['test'].features['label'].names"],"metadata":{"id":"quQJYO3QgOEa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion_matrix(y_preds, y_true, labels):\n","    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n","    fig, ax = plt.subplots(figsize=(6, 6))\n","    labels_for_fig = [l[0:4]+'.' for l in labels]\n","    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n","                                  display_labels=labels_for_fig)\n","    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n","    plt.title(\"Normalized confusion matrix\")\n","    plt.show()\n","\n","plot_confusion_matrix(y_preds, y_valid, labels)"],"metadata":{"id":"nZLbnHYofdkz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["‚ùì [3] Que constatez-vous ? Quelle est la classe pour laquelle les r√©sultats sont les moins bons ? Pourquoi ?"],"metadata":{"id":"EFqCuLEO3TAf"}},{"cell_type":"markdown","source":["Enfin, nous allons analyser les erreurs de classification. Pour cela, nous allons trier les instances par perte d√©croissante."],"metadata":{"id":"t9v18dYP7NLe"}},{"cell_type":"code","source":["from torch.nn.functional import cross_entropy\n","\n","def forward_pass_with_label(batch):\n","    # Fonction qui retourne la perte (entropie crois√©e) et la classe pr√©dite\n","    inputs = {k:v.to(device) for k,v in batch.items()\n","              if k in tokenizer.model_input_names}\n","\n","    with torch.no_grad():\n","        output = model(**inputs)\n","        pred_label = torch.argmax(output.logits, axis=-1)\n","        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n","                             reduction=\"none\")\n","    return {\"loss\": loss.cpu().numpy(),\n","            \"predicted_label\": pred_label.cpu().numpy()}"],"metadata":{"id":"hOtjmEp25XNz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Conversion des donn√©es au bon format\n","tokenized_data.set_format(\"torch\",\n","                            columns=[\"input_ids\", \"attention_mask\", \"label\"])"],"metadata":{"id":"za7_2X-u5fSB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calcul des valeurs de perte\n","tokenized_data[\"test\"] = tokenized_data[\"test\"].map(\n","    forward_pass_with_label, batched=True, batch_size=64)"],"metadata":{"id":"QHF67whN5q15"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Cr√©ation d'un DataFrame avec les textes, les pertes les classe (pr√©dites et attendues)\n","\n","def label_int2str(row):\n","    return tokenized_data[\"train\"].features[\"label\"].int2str(row)\n","\n","tokenized_data.set_format(\"pandas\")\n","cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n","df_test = tokenized_data[\"test\"][:][cols]\n","df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n","df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n","                              .apply(label_int2str))"],"metadata":{"id":"7EgrfZSv53oK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Pour √©viter l'affichage tronqu√© des descriptions\n","pd.set_option('display.max_colwidth', -1)\n","# Affichage des 10 premi√®res instances tri√©es par perte d√©croissante\n","df_test.sort_values(\"loss\", ascending=False).head(10)"],"metadata":{"id":"sFUw5MzR6g-e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Affichage des 10 premi√®res instances tri√©es par perte croissante\n","# Cela permet de voir les instances pour lesquelles les pr√©dictions sont les plus certaines\n","df_test.sort_values(\"loss\", ascending=True).head(10)"],"metadata":{"id":"5Ua_al9E9Sfx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcsLC2U64JmNCsJu9b3Sm9"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}