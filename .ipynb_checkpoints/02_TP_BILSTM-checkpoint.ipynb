{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLkjtwQhZOuSLq9vylKELL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"aS75L2rGYGkE"},"source":["import numpy as np\n","import pandas as pd\n","from sklearn import model_selection\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","from tensorflow.keras.layers import Embedding\n","import seaborn as sns\n","sns.set_theme(style=\"darkgrid\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dd5iy64dYqbD"},"source":["# R√©seau de neurones r√©current LSTM bi-directionnel pour la classification de documents\n","\n"," > ‚ÑπÔ∏è Inspir√© de :\n"," > - https://keras.io/examples/nlp/pretrained_word_embeddings/\n"," > - https://keras.io/examples/nlp/bidirectional_lstm_imdb/\n"," > - https://www.machinecurve.com/index.php/2020/02/18/how-to-use-k-fold-cross-validation-with-keras/\n","\n","<div class=\"alert alert-block alert-info\">\n","\n","ü•Ö **Objectifs**\n","\n","- Savoir utiliser `keras` pour faire de l'apprentissage supervis√© √† partir de documents avec des r√©seaux de neurones r√©currents de type LSTM bi-directionnel\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"iorMXir5bcGh"},"source":["## 1. Chargement des donn√©es (corpus et plongements)\n","\n","Nous allons r√©-utiliser la proc√©dure vue dans le TP pr√©c√©dent."]},{"cell_type":"markdown","source":["### 1.1. Corpus annot√©"],"metadata":{"id":"VHRrZJr0J6T-"}},{"cell_type":"code","metadata":{"id":"gGTILZn1b1BK"},"source":["!mkdir data\n","!wget -P data https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/winemag-fr_train.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"92jiRstRcBPA"},"source":["# Lecture du fichier CSV\n","wine_df = pd.read_csv(\"data/winemag-fr_train.csv\", sep=\",\", dtype={'description': 'object',\n","                                           'price': 'float64',\n","                                           'province': 'category',\n","                                           'variety': 'object'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ErLz9G_YuSAw"},"source":["# Liste des classes\n","class_names = sorted(wine_df.province.unique().categories.to_list())\n","print(\"Classes :\", class_names)\n","print(\"Nombre d'exemplaires :\", len(wine_df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FGYQP_lIzJBP"},"source":["# On associe √† chaque classe un identifiant unique\n","class_index = {class_names[i]:i for i in range(len(class_names))}\n","class_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W1ReFEDadAgl"},"source":["# On utilise uniquement la vari√©t√© et les descriptions comme donn√©es d'entr√©e\n","X_train_variety = wine_df.variety.str.split('_')\n","X_train = X_train_variety.str.join(' ') + ' ' + wine_df.description\n","# Les noms des classes sont remplac√©es par leur identifiant (un entier positif)\n","y_train = wine_df.province.map(class_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train.head()"],"metadata":{"id":"JazhsYbNvxhu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_train.head()"],"metadata":{"id":"O_iXDMCvv22m"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n9xam7ZddKlp"},"source":["def get_vectorizer(documents, max_voc_size=8000, max_seq_length=50, batch_size=128):\n","  vectorizer = TextVectorization(max_tokens=max_voc_size,\n","                                 output_sequence_length=max_seq_length)\n","  # Cr√©ation du jeu de donn√©es √† partir de X_train et constitution de lots de 128 instances\n","  text_ds = tf.data.Dataset.from_tensor_slices(documents).batch(batch_size)\n","  # Cr√©ation du vocabulaire √† partir des donn√©es d'entr√©e\n","  vectorizer.adapt(text_ds)\n","  return vectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keras_vectorizer = get_vectorizer(X_train)"],"metadata":{"id":"9h_Qx8RKwYCH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPLthMLvc5X-"},"source":["Vocabulaire obtenu :"]},{"cell_type":"code","metadata":{"id":"nTuTysVWrfxB"},"source":["voc = keras_vectorizer.get_vocabulary()\n","print(len(voc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Je27iqIIrpjp"},"source":["word_index = dict(zip(voc, range(len(voc))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sG9v-0Ggob13"},"source":["### 1.2. Plongements de mots pr√©-entra√Æn√©s\n"]},{"cell_type":"code","metadata":{"id":"uP2iPaXmp29V"},"source":["!wget -P data https://git.unistra.fr/dbernhard/ftaa_data/-/raw/main/model_26.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uD3eYGzPrBuB"},"source":["def load_embeddings(embeddings_file):\n","  embeddings_index = {}\n","  with open(embeddings_file, 'r', encoding='utf8') as f:\n","      for line in f:\n","          word, coefs = line.split(maxsplit=1)\n","          coefs = np.fromstring(coefs, \"f\", sep=\" \")\n","          embeddings_index[word] = coefs\n","  print(f'{len(embeddings_index)} vecteurs de mots ont √©t√© lus')\n","  return embeddings_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Chargement des plongements du fichier model_6.txt\n","m26_embeddings = load_embeddings('data/model_26.txt')"],"metadata":{"id":"0QPQWmolxUcJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"inAbw5W1rXFx"},"source":["def get_embedding_matrix(vocabulary, embeddings_index, embedding_dim = 300):\n","  num_tokens = len(vocabulary)\n","  hits = 0\n","  misses = 0\n","\n","  # Pr√©paration de la matrice\n","  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n","  for word, i in word_index.items():\n","      embedding_vector = embeddings_index.get(word)\n","      if embedding_vector is not None:\n","          embedding_matrix[i] = embedding_vector\n","          hits += 1\n","      else:\n","          misses += 1\n","  print(f'{hits} mots ont √©t√© trouv√©s dans les plongements pr√©-entra√Æn√©s')\n","  print(f'{misses} sont absents')\n","  return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Construction de la matrice de plongements √† partir du vocabulaire\n","m26_embedding_matrix = get_embedding_matrix(voc, m26_embeddings)"],"metadata":{"id":"2OEWxdkYx3GW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M0DJAj8PsnHl"},"source":["## 2. Construction et entra√Ænement du mod√®le\n","\n","\n","Nous allons faire une validation crois√©e √† 5 plis.\n","\n","Le r√©seau de neurones comprendra deux couches de LSTM bi-directionnels"]},{"cell_type":"code","source":["def get_biLSTM_model(voc_size, embedding_matrix, embedding_dim=300):\n","  # Cr√©ation du mod√®le\n","  int_sequences_input = keras.Input(shape=(None,), dtype=\"int64\")\n","  embedding_layer = Embedding(voc_size, embedding_dim, trainable=True,\n","      embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n","  )\n","\n","  embedded_sequences = embedding_layer(int_sequences_input)\n","  x = layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2))(embedded_sequences)\n","  preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n","  model = keras.Model(int_sequences_input, preds)\n","  return model"],"metadata":{"id":"8yjEq0ErykRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Affichage de l'architecture du mod√®le\n","biLSTM_model = get_biLSTM_model(len(voc), m26_embedding_matrix)\n","biLSTM_model.summary()"],"metadata":{"id":"smpHUUqwzf5j"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rQD_IZTzcsjB"},"source":["# Fonction pour l'entra√Ænement d'un mod√®le\n","def train_model(X, y, model_function, vectorizer,\n","                voc_size, embedding_matrix, embedding_dim=300, batch_size=128):\n","  # Listes utilis√©es pour sauvegarder les r√©sultats obtenus √† chaque pli\n","  acc_per_fold = []\n","  loss_per_fold = []\n","  histories = []\n","  folds = 5\n","  stratkfold = model_selection.StratifiedKFold(n_splits=folds, shuffle=True,\n","                                              random_state=12)\n","  fold_no = 1\n","  for train, test in stratkfold.split(X, y):\n","    m_function = globals()[model_function]\n","    model = m_function(voc_size, embedding_matrix, embedding_dim)\n","\n","    print('------------------------------------------------------------------------')\n","    print(f'Entra√Ænement pour le pli {fold_no} ...')\n","    fold_x_train = vectorizer(X.iloc[train].to_numpy()).numpy()\n","    fold_x_val = vectorizer(X.iloc[test].to_numpy()).numpy()\n","    fold_y_train = y.iloc[train].to_numpy()\n","    fold_y_val = y.iloc[test].to_numpy()\n","\n","    # Compilation du mod√®le : permet de pr√©ciser la fonction de perte et l'optimiseur\n","    # loss=sparse_categorical_crossentropy : entropie crois√©e, dans le cas o√π les\n","    #  classes cibles sont indiqu√©es sous forme d'entiers. Il s'agira de minimiser\n","    #  la perte pendant l'apprentissage\n","    # optimizer=rmsprop : l'optimiseur d√©termine la mani√®re doit les poids seront\n","    #  mis √† jour pendant l'apprentissage\n","    model.compile(\n","      loss=\"sparse_categorical_crossentropy\", optimizer=\"rmsprop\", metrics=[\"acc\"]\n","    )\n","    # Entra√Ænement\n","    history = model.fit(fold_x_train, fold_y_train, batch_size=batch_size,\n","                        epochs=10, validation_data=(fold_x_val, fold_y_val))\n","    histories.append(history)\n","    # Evaluation sur les donn√©es de validation\n","    scores = model.evaluate(fold_x_val, fold_y_val, verbose=0)\n","    print(f'Scores pour le pli {fold_no}: {model.metrics_names[0]} = {scores[0]:.2f};',\n","          f'{model.metrics_names[1]} = {scores[1]*100:.2f}%')\n","    acc_per_fold.append(scores[1] * 100)\n","    loss_per_fold.append(scores[0])\n","    fold_no = fold_no + 1\n","\n","  # Affichage des scores moyens par pli\n","  print('---------------------------------------------------------------------')\n","  print('Scores par pli')\n","  for i in range(0, len(acc_per_fold)):\n","    print('---------------------------------------------------------------------')\n","    print(f'> Pli {i+1} - Loss: {loss_per_fold[i]:.2f}',\n","          f'- Accuracy: {acc_per_fold[i]:.2f}%')\n","  print('---------------------------------------------------------------------')\n","  print('Scores moyens pour tous les plis :')\n","  print(f'> Accuracy: {np.mean(acc_per_fold):.2f}',\n","        f'(+- {np.std(acc_per_fold):.2f})')\n","  print(f'> Loss: {np.mean(loss_per_fold):.2f}')\n","  print('---------------------------------------------------------------------')\n","  return histories"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entra√Ænement du mod√®le et r√©cup√©ration des r√©sultats\n","biLSTM_histories = train_model(X_train, y_train, 'get_biLSTM_model',\n","                            keras_vectorizer, len(voc), m26_embedding_matrix)"],"metadata":{"id":"Y1t44LY3Akyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7imQ26Is9JGL"},"source":["Affichage des r√©sultats sous forme graphique :"]},{"cell_type":"code","metadata":{"id":"9lEJ4lW49DQr"},"source":["def plot_results(histories):\n","  accuracy_data = []\n","  loss_data = []\n","  for i, h in enumerate(histories):\n","    acc = h.history['acc']\n","    val_acc = h.history['val_acc']\n","    loss = h.history['loss']\n","    val_loss = h.history['val_loss']\n","    for j in range(len(acc)):\n","      accuracy_data.append([i+1, j+1, acc[j], 'Entra√Ænement'])\n","      accuracy_data.append([i+1, j+1, val_acc[j], 'Validation'])\n","      loss_data.append([i+1, j+1, loss[j], 'Entra√Ænement'])\n","      loss_data.append([i+1, j+1, val_loss[j], 'Validation'])\n","\n","  acc_df = pd.DataFrame(accuracy_data,\n","                        columns=['Pli', 'Epoch', 'Accuracy', 'Donn√©es'])\n","  sns.relplot(data=acc_df, x='Epoch', y='Accuracy', hue='Pli', style='Donn√©es',\n","              kind='line')\n","\n","  loss_df = pd.DataFrame(loss_data, columns=['Pli', 'Epoch', 'Perte', 'Donn√©es'])\n","  sns.relplot(data=loss_df, x='Epoch', y='Perte', hue='Pli', style='Donn√©es',\n","              kind='line')"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_results(biLSTM_histories)"],"metadata":{"id":"ySo3Hei2CuLh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V0SyRAvzcsZJ"},"source":["‚ùì [1] Que constatez-vous par rapport aux r√©sultats obtenus pr√©c√©demment pour ce jeu de donn√©es (tf-idf) ?"]},{"cell_type":"markdown","source":["‚ùì [2] Remplacez la couche LSTM par une couche de type GRU. Que constatez-vous pour les r√©sultats ?"],"metadata":{"id":"X_0eeyIsUF_-"}},{"cell_type":"code","source":[],"metadata":{"id":"NriSHnAAdZ2U"},"execution_count":null,"outputs":[]}]}